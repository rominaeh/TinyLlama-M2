# TinyLlama-M2

Run TinyLlama (1.1B) locally on Apple Silicon using Python.  
This repository demonstrates how to run a local LLM using `llama-cpp-python`.

---

## Requirements

- Python 3.10+
- `llama-cpp-python`
- Internet connection (to download the model)


=== My Hardware Info ===
System: Darwin
Machine: arm64
CPU Architecture: arm
CPU Model: Apple M2
CPU Cores: 8 physical, 8 logical
RAM: 8 GB
=====================


Install dependencies via:

```bash
pip install -r requirements.txt

# TinyLlama-M2

Run TinyLlama (1.1B) locally on Apple Silicon using Python.  
This repository demonstrates how to run a local LLM using `llama-cpp-python`.

---

## Requirements

- Python 3.10+
- `llama-cpp-python`
- Internet connection (to download the model)

Install dependencies via:

```bash
pip install -r requirements.txt

=== My Hardware Info ===
System: Darwin
Machine: arm64
CPU Architecture: arm
CPU Model: Apple M2
CPU Cores: 8 physical, 8 logical
RAM: 8 GB
=====================
